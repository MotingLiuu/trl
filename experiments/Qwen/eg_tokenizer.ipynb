{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b273cd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/satori_hdd1/mutyuu/miniconda3/envs/align_trl/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import List, Dict, Tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f602a913",
   "metadata": {},
   "source": [
    "# step1 apply_chat_template example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6905cef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompts(jsonl_path):\n",
    "    \"\"\"Load prompts and references from JSONL file.\"\"\"\n",
    "    prompts = []\n",
    "    \n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            data = json.loads(line.strip())\n",
    "            data.pop('completion', None)\n",
    "            prompts.append(data)\n",
    "            \n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16c65b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = load_prompts('test_conversational.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deccc9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_example = prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48532ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Example:\n",
      " {'prompt': [{'role': 'user', 'content': 'SUBREDDIT: r/relationships\\nTITLE: Me [19 F] with my friend [19 M], not sure if I may have messed things up already.\\nPOST: Hello hello everybody. I hope this isn\\'t too trivial of a question to ask on here, but I\\'ve been feeling a bit out of my depth when it comes to this situation (I\\'ve had only one relationship before, and for many reasons, it was out of the ordinary).\\n\\nOkay! So, a couple of weeks ago, I started talking to this guy on Facebook, through a student group that we were both part of. I thought he was sort of cute, so I sent him a PM just to talk, etc, etc. We\\'re both transfer students at the same school, so I knew that we could eventually meet in person once we both moved on-campus. So, we did, and we hung out maybe twice, just as friends.\\n\\nOkay. So, everything is going pretty well. We talk over Facebook and Snapchat, whatever. So, Saturday night, I was just hanging out with people and kind of being bored, when I got a Snapchat from him asking what I was doing. I asked if he wanted to hang out, so we did. \\n\\nWe ended up smoking pot (the first time for me, ever), and sort of just wandering around. Eventually we ended up back at his dorm room, where high me decided to just go for it, and I came on to him pretty strongly. It worked out for me (luckily, otherwise things would have been really super awkward), and we ended up messing around but not having sex.\\n\\nYesterday, however, I ended up going to hang out with him again, and this time we did sleep together. Afterward, we kind of discussed what we were going to do, and he just said that he wanted to \"play it by ear\" and not slap any labels on anything. I\\'m wondering if this means that he wants a fwb-type situation, or if he might actually be interested in me. The way I\\'ve been acting is extremely out of character for me, and I am not interested in having a fuck buddy. I like him, and I would be very interested in maybe seeing where things go, but I\\'m worried that I may have ruined my chances of a relationship by sleeping with him already.\\nTL;DR: '}]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Prompt Example:\\n\", prompt_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f401f197",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-1.7B\",\n",
    "                                          trust_remote_code=True,\n",
    "                                          use_fast=True\n",
    "                                          )\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cae3def9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt with Chat Template:\n",
      " <|im_start|>user\n",
      "SUBREDDIT: r/relationships\n",
      "TITLE: Me [19 F] with my friend [19 M], not sure if I may have messed things up already.\n",
      "POST: Hello hello everybody. I hope this isn't too trivial of a question to ask on here, but I've been feeling a bit out of my depth when it comes to this situation (I've had only one relationship before, and for many reasons, it was out of the ordinary).\n",
      "\n",
      "Okay! So, a couple of weeks ago, I started talking to this guy on Facebook, through a student group that we were both part of. I thought he was sort of cute, so I sent him a PM just to talk, etc, etc. We're both transfer students at the same school, so I knew that we could eventually meet in person once we both moved on-campus. So, we did, and we hung out maybe twice, just as friends.\n",
      "\n",
      "Okay. So, everything is going pretty well. We talk over Facebook and Snapchat, whatever. So, Saturday night, I was just hanging out with people and kind of being bored, when I got a Snapchat from him asking what I was doing. I asked if he wanted to hang out, so we did. \n",
      "\n",
      "We ended up smoking pot (the first time for me, ever), and sort of just wandering around. Eventually we ended up back at his dorm room, where high me decided to just go for it, and I came on to him pretty strongly. It worked out for me (luckily, otherwise things would have been really super awkward), and we ended up messing around but not having sex.\n",
      "\n",
      "Yesterday, however, I ended up going to hang out with him again, and this time we did sleep together. Afterward, we kind of discussed what we were going to do, and he just said that he wanted to \"play it by ear\" and not slap any labels on anything. I'm wondering if this means that he wants a fwb-type situation, or if he might actually be interested in me. The way I've been acting is extremely out of character for me, and I am not interested in having a fuck buddy. I like him, and I would be very interested in maybe seeing where things go, but I'm worried that I may have ruined my chances of a relationship by sleeping with him already.\n",
      "TL;DR: <|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Prompt tokenized:\n",
      "  {'input_ids': [151644, 872, 198, 29038, 787, 4103, 952, 25, 435, 14, 85824, 198, 50328, 25, 2157, 508, 16, 24, 434, 60, 448, 847, 4238, 508, 16, 24, 386, 1125, 537, 2704, 421, 358, 1231, 614, 64202, 2513, 705, 2669, 624, 2946, 25, 21927, 23811, 16083, 13, 358, 3900, 419, 4436, 944, 2238, 35647, 315, 264, 3405, 311, 2548, 389, 1588, 11, 714, 358, 3003, 1012, 8266, 264, 2699, 700, 315, 847, 7990, 979, 432, 4041, 311, 419, 6534, 320, 40, 3003, 1030, 1172, 825, 5025, 1573, 11, 323, 369, 1657, 7966, 11, 432, 572, 700, 315, 279, 19119, 3593, 32313, 0, 2055, 11, 264, 5625, 315, 5555, 4134, 11, 358, 3855, 7404, 311, 419, 7412, 389, 5573, 11, 1526, 264, 5458, 1874, 429, 582, 1033, 2176, 949, 315, 13, 358, 3381, 566, 572, 3378, 315, 18838, 11, 773, 358, 3208, 1435, 264, 5851, 1101, 311, 3061, 11, 4992, 11, 4992, 13, 1205, 2299, 2176, 8317, 4143, 518, 279, 1852, 2906, 11, 773, 358, 6876, 429, 582, 1410, 9583, 3367, 304, 1697, 3055, 582, 2176, 7726, 389, 93319, 13, 2055, 11, 582, 1521, 11, 323, 582, 18295, 700, 7196, 10917, 11, 1101, 438, 4780, 382, 32313, 13, 2055, 11, 4297, 374, 2087, 5020, 1632, 13, 1205, 3061, 916, 5573, 323, 55179, 11, 8820, 13, 2055, 11, 7728, 3729, 11, 358, 572, 1101, 20704, 700, 448, 1251, 323, 3093, 315, 1660, 33286, 11, 979, 358, 2684, 264, 55179, 504, 1435, 10161, 1128, 358, 572, 3730, 13, 358, 4588, 421, 566, 4829, 311, 14678, 700, 11, 773, 582, 1521, 13, 4710, 1654, 9482, 705, 19578, 3338, 320, 1782, 1156, 882, 369, 752, 11, 3512, 701, 323, 3378, 315, 1101, 53963, 2163, 13, 37174, 582, 9482, 705, 1182, 518, 806, 29109, 3054, 11, 1380, 1550, 752, 6635, 311, 1101, 728, 369, 432, 11, 323, 358, 3697, 389, 311, 1435, 5020, 16510, 13, 1084, 6439, 700, 369, 752, 320, 85376, 1541, 11, 5937, 2513, 1035, 614, 1012, 2167, 2256, 28759, 701, 323, 582, 9482, 705, 75751, 2163, 714, 537, 3432, 1839, 382, 50277, 11, 4764, 11, 358, 9482, 705, 2087, 311, 14678, 700, 448, 1435, 1549, 11, 323, 419, 882, 582, 1521, 6084, 3786, 13, 4636, 1606, 11, 582, 3093, 315, 14078, 1128, 582, 1033, 2087, 311, 653, 11, 323, 566, 1101, 1053, 429, 566, 4829, 311, 330, 1363, 432, 553, 2430, 1, 323, 537, 50052, 894, 9201, 389, 4113, 13, 358, 2776, 20293, 421, 419, 3363, 429, 566, 6801, 264, 282, 20211, 10604, 6534, 11, 476, 421, 566, 2578, 3520, 387, 8014, 304, 752, 13, 576, 1616, 358, 3003, 1012, 15358, 374, 9016, 700, 315, 3668, 369, 752, 11, 323, 358, 1079, 537, 8014, 304, 3432, 264, 7820, 36672, 13, 358, 1075, 1435, 11, 323, 358, 1035, 387, 1602, 8014, 304, 7196, 9120, 1380, 2513, 728, 11, 714, 358, 2776, 17811, 429, 358, 1231, 614, 46068, 847, 16963, 315, 264, 5025, 553, 21127, 448, 1435, 2669, 624, 13470, 26, 7687, 25, 220, 151645, 198, 151644, 77091, 198], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "prompt_apply_template = tokenizer.apply_chat_template(\n",
    "    prompt_example['prompt'],  \n",
    "    truncation=\"True\",\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "prompt_tokenized = tokenizer(\n",
    "    prompt_apply_template,\n",
    ")\n",
    "print(\"Prompt with Chat Template:\\n\", prompt_apply_template)\n",
    "print(\"Prompt tokenized:\\n \", prompt_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b581a9cc",
   "metadata": {},
   "source": [
    "# step2 process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "539a2a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "TRAIN_DATA = \"../../research/alignment/data/TLDR/sft/train_conversational.jsonl\"\n",
    "TEST_DATA = \"../../research/alignment/data/TLDR/sft/test_conversational.jsonl\"\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        \"train\": TRAIN_DATA,\n",
    "        \"test\": TEST_DATA\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21d8f39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt'],\n",
      "        num_rows: 116722\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt'],\n",
      "        num_rows: 6553\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.remove_columns(\"completion\")\n",
    "print(f\"dataset: {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d06f9e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chat(item):\n",
    "    formatted_chat = tokenizer.apply_chat_template(\n",
    "        item[\"prompt\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    \n",
    "    tokenized_output = tokenizer(\n",
    "        formatted_chat,\n",
    "        truncation=True,\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        padding=False,\n",
    "    )\n",
    "    \n",
    "    return tokenized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "76676bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=36): 100%|██████████| 116722/116722 [00:05<00:00, 19929.01 examples/s]\n",
      "Map (num_proc=36): 100%|██████████| 6553/6553 [00:02<00:00, 2657.61 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 116722\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 6553\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(\n",
    "    process_chat,\n",
    "    batch_size=True,\n",
    "    num_proc=36,\n",
    "    remove_columns=[\"prompt\"],\n",
    ")\n",
    "print(f\"dataset: {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5fdae2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_inputs: {'input_ids': [[151644, 872, 198, 29038, 787, 4103, 952, 25, 435, 14, 85824, 198, 50328, 25, 358, 320, 69, 14, 17, 17, 8, 614, 311, 7071, 700, 421, 358, 1366, 311, 2058, 1414, 1493, 7571, 476, 537, 323, 1035, 12213, 311, 5112, 67092, 198, 2946, 25, 2806, 2704, 421, 419, 17180, 1588, 714, 432, 594, 5802, 264, 1430, 13, 4710, 3707, 26485, 510, 4498, 358, 320, 69, 14, 17, 17, 8, 3937, 1526, 847, 1156, 1931, 84498, 220, 17, 1635, 4134, 1576, 566, 4362, 3550, 1283, 264, 1042, 315, 4924, 926, 437, 220, 432, 88489, 752, 803, 1091, 358, 3381, 13, 1084, 572, 264, 27102, 882, 304, 847, 2272, 4152, 311, 5382, 448, 847, 6554, 323, 5499, 3432, 279, 6012, 311, 3931, 1059, 700, 315, 847, 2272, 13, 358, 646, 16698, 1576, 315, 432, 572, 458, 14269, 35750, 323, 419, 7412, 572, 15175, 323, 3207, 944, 1414, 1246, 311, 3484, 448, 752, 13, 1205, 9482, 553, 1435, 30426, 369, 264, 2254, 476, 773, 1283, 2087, 311, 264, 18780, 448, 847, 4780, 13, 3197, 358, 1744, 1182, 358, 6426, 566, 1101, 9482, 13, 2055, 1283, 566, 9482, 432, 3694, 847, 18210, 358, 16256, 714, 847, 4780, 8910, 752, 1526, 432, 323, 358, 2684, 9279, 315, 4297, 504, 1435, 3156, 448, 14376, 3645, 13, 4710, 7039, 25, 11445, 1012, 4558, 220, 18, 1635, 1431, 323, 358, 3003, 17019, 2664, 1283, 82681, 323, 23034, 7147, 38200, 1783, 13, 3017, 6554, 702, 1012, 700, 315, 847, 2272, 2474, 1221, 773, 1052, 594, 1012, 56238, 315, 5098, 13, 20690, 16245, 1283, 6832, 1045, 18366, 1052, 1012, 803, 20017, 911, 429, 882, 315, 847, 2272, 714, 979, 358, 1490, 1435, 476, 264, 6802, 4297, 4041, 1182, 13, 576, 21261, 323, 18923, 4446, 752, 1182, 1495, 13, 4710, 15986, 4780, 320, 21028, 7571, 8, 525, 389, 847, 22943, 1576, 582, 633, 3156, 1632, 892, 374, 2588, 311, 1477, 323, 358, 1414, 807, 3278, 2677, 614, 806, 1182, 13, 1988, 9120, 1435, 304, 264, 6802, 476, 7404, 311, 1435, 518, 264, 21277, 3432, 264, 10435, 374, 11045, 13, 356, 27509, 16877, 315, 847, 1482, 25838, 374, 2494, 358, 1366, 311, 5648, 13, 4710, 4416, 358, 3003, 1012, 7274, 429, 358, 614, 311, 3931, 3645, 448, 1493, 7571, 1576, 432, 594, 882, 311, 3271, 389, 1576, 432, 594, 38245, 13, 1084, 594, 1850, 311, 5648, 1435, 438, 1632, 13, 1988, 686, 807, 387, 26132, 291, 30, 4841, 807, 4193, 432, 30, 2160, 1052, 2087, 311, 387, 28759, 2090, 30, 358, 2776, 537, 2704, 421, 432, 594, 279, 1290, 311, 653, 323, 1410, 990, 1045, 4889, 17979, 624, 13470, 26, 7687, 25, 220, 151645, 198, 151644, 77091, 198], [151644, 872, 198, 29038, 787, 4103, 952, 25, 435, 14, 1469, 275, 198, 50328, 25, 17481, 609, 3043, 53, 0, 55378, 389, 10282, 389, 624, 2946, 25, 220, 18, 15, 37, 11, 220, 20, 6, 21, 3263, 13387, 25, 220, 17, 18, 21, 41253, 25, 220, 16, 20, 15, 40035, 25, 220, 17, 16, 24, 271, 40, 17529, 7037, 17059, 323, 6629, 7037, 15088, 13, 358, 4172, 4201, 264, 64288, 279, 1537, 3040, 5555, 476, 773, 1380, 358, 572, 15700, 518, 220, 17, 17, 17, 13, 434, 3818, 1075, 3093, 315, 264, 293, 62273, 11, 714, 6876, 432, 594, 1576, 358, 8990, 944, 1012, 438, 7304, 438, 358, 1265, 448, 847, 9968, 11, 323, 279, 1537, 2003, 323, 264, 4279, 614, 1012, 14264, 448, 2272, 2513, 11, 773, 358, 8990, 944, 1012, 50482, 438, 13814, 438, 358, 3003, 17019, 1483, 311, 13, 3197, 358, 46612, 7037, 438, 4622, 389, 7014, 11, 358, 572, 3093, 315, 24402, 311, 1490, 279, 5452, 537, 36708, 3173, 323, 24692, 432, 572, 882, 311, 79995, 1495, 1549, 323, 2167, 3736, 847, 9968, 13, 11201, 572, 847, 6629, 3419, 1899, 11, 323, 358, 3003, 6476, 1560, 33917, 304, 4586, 2474, 7014, 1576, 358, 10568, 1045, 15138, 54046, 2899, 813, 9876, 916, 279, 9001, 13, 358, 633, 389, 279, 5452, 1112, 275, 2727, 220, 17, 16, 24, 13, 1232, 28458, 266, 30, 358, 1896, 847, 21595, 11, 892, 525, 1495, 10078, 504, 1537, 2254, 11, 323, 448, 458, 2790, 9350, 4709, 315, 220, 23, 14924, 504, 847, 5916, 1459, 389, 220, 16, 17, 14, 17, 18, 14, 16, 19, 0, 4329, 315, 847, 15097, 614, 1012, 8266, 264, 2699, 775, 23438, 438, 315, 3309, 323, 1431, 358, 1414, 432, 594, 1101, 537, 304, 847, 1968, 13, 358, 2776, 1431, 279, 3100, 477, 323, 24632, 358, 3003, 1012, 2474, 1290, 2163, 1550, 2906, 4894, 13470, 26, 7687, 25, 220, 151645, 198, 151644, 77091, 198], [151644, 872, 198, 29038, 787, 4103, 952, 25, 435, 14, 85824, 198, 50328, 25, 2157, 508, 16, 24, 37, 60, 448, 847, 4238, 508, 16, 24, 44, 60, 220, 16, 15, 3951, 11, 758, 325, 30602, 481, 6928, 476, 24647, 5267, 2946, 25, 3555, 525, 697, 7343, 911, 53582, 30602, 498, 3003, 1030, 304, 3267, 11871, 30, 2585, 614, 498, 25934, 448, 1105, 11, 7945, 279, 6174, 429, 498, 646, 944, 10265, 1939, 40, 2776, 537, 5023, 304, 264, 5025, 11, 714, 5926, 358, 3003, 15043, 429, 1052, 374, 4325, 879, 13151, 752, 11, 323, 358, 2776, 8014, 304, 1105, 11, 2238, 13, 9270, 398, 11, 279, 1172, 2874, 358, 2776, 537, 10161, 1105, 700, 374, 1576, 358, 1414, 429, 358, 614, 1045, 53582, 30602, 429, 1184, 311, 387, 6439, 1526, 481, 7945, 304, 279, 21889, 315, 2487, 2168, 13, 5976, 358, 2776, 16506, 304, 279, 2732, 315, 847, 2487, 11, 358, 3003, 1030, 17478, 11, 24607, 45805, 2176, 389, 847, 11715, 323, 36349, 2474, 358, 572, 1602, 3908, 13, 1084, 594, 264, 3281, 943, 448, 902, 4583, 26116, 11, 714, 16026, 11827, 429, 358, 2506, 847, 6787, 297, 2181, 3080, 432, 5780, 3123, 320, 53210, 2090, 29018, 973, 432, 568, 9211, 315, 419, 432, 594, 537, 773, 1753, 3042, 14584, 438, 3460, 26968, 315, 60599, 525, 382, 27989, 358, 8809, 4325, 911, 419, 64351, 1573, 4113, 7244, 30, 18885, 358, 1101, 1077, 432, 12761, 1105, 979, 279, 15097, 2525, 1007, 30, 3155, 358, 3291, 1105, 330, 10061, 594, 2506, 389, 847, 15478, 369, 1431, 1, 1393, 582, 653, 1039, 2562, 30, 4710, 12116, 498, 1030, 11449, 448, 4113, 4428, 30, 358, 1366, 311, 6723, 1246, 807, 3937, 4894, 13470, 26, 7687, 25, 220, 151645, 198, 151644, 77091, 198], [151644, 872, 198, 29038, 787, 4103, 952, 25, 435, 56461, 3104, 24387, 198, 50328, 25, 31399, 26310, 5458, 11636, 476, 13997, 369, 1495, 8160, 5267, 2946, 25, 358, 614, 400, 17, 20, 74, 304, 5458, 11636, 13, 3776, 869, 11679, 518, 220, 24, 13, 20, 4, 320, 74154, 10619, 13900, 8, 323, 11627, 3800, 6775, 1948, 220, 18, 13, 19, 4, 323, 220, 21, 13, 23, 14360, 30925, 8160, 817, 2254, 2790, 374, 400, 18, 15, 16, 13, 16, 21, 13, 6065, 279, 1790, 220, 24, 3951, 11, 358, 686, 2291, 1007, 400, 16, 16, 74, 315, 1493, 11, 892, 686, 633, 9279, 315, 4297, 3403, 220, 20, 4, 2734, 323, 686, 5943, 279, 2790, 8028, 8160, 311, 400, 16, 20, 15, 13, 4710, 1655, 279, 835, 315, 279, 220, 24, 3951, 11, 1039, 18986, 686, 387, 2163, 400, 18, 20, 74, 13, 2411, 429, 882, 847, 9972, 686, 1184, 311, 7627, 264, 1803, 773, 1045, 315, 429, 686, 387, 806, 1495, 8160, 13, 2055, 803, 88616, 400, 17, 20, 12, 18, 15, 74, 13, 4710, 49882, 4107, 304, 279, 3853, 11, 1948, 264, 1042, 311, 1378, 1635, 504, 1431, 11, 847, 9972, 323, 358, 1231, 387, 7218, 13, 72925, 3175, 2997, 10415, 304, 419, 3082, 728, 369, 2163, 400, 18, 15, 15, 74, 13, 4710, 1655, 279, 835, 315, 279, 220, 24, 3951, 11, 1265, 358, 3060, 311, 5244, 389, 12515, 1495, 5458, 11636, 320, 8206, 686, 387, 264, 8172, 315, 400, 16, 19, 74, 553, 1221, 8, 476, 7826, 1039, 18986, 79067, 8160, 30, 358, 614, 400, 20, 17, 15, 15, 3183, 78, 311, 16925, 6718, 1948, 11636, 323, 1495, 8160, 323, 358, 2776, 537, 2704, 1246, 1850, 311, 22089, 432, 624, 13470, 26, 7687, 25, 220, 151645, 198, 151644, 77091, 198], [151644, 872, 198, 29038, 787, 4103, 952, 25, 435, 14, 85824, 198, 50328, 25, 3017, 58, 17, 20, 76, 60, 22761, 508, 17, 19, 69, 60, 374, 1172, 6419, 323, 23795, 979, 358, 2776, 53166, 1055, 323, 28727, 13, 320, 24, 3951, 340, 2946, 25, 31345, 13757, 271, 40, 13686, 279, 803, 358, 2776, 9255, 323, 28727, 6974, 847, 22761, 11, 279, 803, 23795, 1340, 9044, 13, 2932, 3278, 2525, 916, 323, 4240, 847, 13154, 11, 653, 34089, 11, 25779, 323, 4296, 369, 752, 11, 1496, 438, 3041, 438, 311, 3010, 20655, 53847, 1393, 358, 2776, 16163, 264, 12883, 0, 4710, 7941, 4977, 6587, 6247, 323, 2213, 2337, 419, 882, 11, 892, 3643, 752, 6247, 323, 358, 17712, 1366, 311, 653, 2513, 1182, 369, 1059, 13, 1634, 5135, 438, 358, 1191, 3730, 1059, 53847, 11, 1340, 21895, 27433, 323, 88958, 2477, 9495, 13, 28157, 4265, 572, 358, 8900, 311, 1896, 1059, 323, 1059, 3368, 311, 13856, 13, 2932, 8604, 7086, 752, 17176, 911, 1246, 358, 2776, 2087, 311, 387, 10164, 2238, 1753, 882, 448, 847, 10641, 320, 14623, 594, 16721, 369, 264, 2003, 5135, 701, 892, 1340, 572, 12473, 6915, 448, 979, 358, 572, 1660, 28727, 448, 1059, 13, 2932, 3278, 1618, 752, 264, 39430, 304, 264, 57034, 1616, 11, 323, 1101, 1896, 279, 39940, 700, 315, 752, 15356, 358, 2776, 3093, 476, 728, 700, 315, 847, 1616, 311, 36879, 13, 4710, 1986, 17712, 3643, 752, 2666, 9255, 323, 78908, 8841, 1059, 13, 9646, 1340, 37107, 429, 11, 1340, 594, 678, 911, 3259, 752, 279, 98852, 25838, 323, 20419, 4756, 369, 678, 279, 17176, 1340, 572, 7086, 752, 279, 2003, 8597, 13, 1084, 594, 264, 42410, 10775, 714, 358, 2776, 537, 2704, 1128, 311, 653, 1588, 13, 358, 3003, 7117, 419, 705, 448, 1059, 323, 1340, 44699, 432, 323, 702, 902, 6291, 13, 2932, 1101, 330, 1859, 2010, 21303, 6974, 752, 7025, 1, 323, 646, 944, 10339, 432, 382, 334, 4416, 1128, 653, 358, 653, 1588, 30, 3155, 358, 2506, 705, 279, 53166, 1055, 11, 28727, 18915, 311, 2506, 1059, 8014, 476, 7676, 1059, 4184, 3173, 304, 44872, 11, 847, 1638, 6243, 624, 13470, 26, 7687, 25, 220, 151645, 198, 151644, 77091, 198], [151644, 872, 198, 29038, 787, 4103, 952, 25, 435, 14, 85824, 198, 50328, 25, 2157, 220, 17, 23, 434, 448, 7412, 358, 2776, 4924, 220, 18, 15, 386, 481, 220, 16, 2254, 11, 2160, 432, 16910, 311, 2548, 421, 4297, 374, 16910, 476, 1079, 358, 1660, 4484, 88, 5267, 2946, 25, 358, 614, 1012, 4924, 419, 7412, 369, 220, 16, 2254, 1431, 323, 4297, 572, 2244, 3080, 1537, 7270, 979, 358, 13686, 566, 5710, 944, 29338, 311, 847, 1467, 448, 279, 1852, 1320, 16531, 11, 18838, 11, 41602, 1467, 429, 566, 5990, 1035, 13, 1260, 1483, 311, 1618, 752, 8770, 11, 40246, 714, 2474, 1537, 7270, 566, 702, 1053, 6857, 315, 1846, 4244, 323, 67994, 702, 1012, 2686, 323, 10535, 702, 1012, 2686, 382, 1519, 18218, 752, 916, 1537, 3729, 311, 3736, 264, 5700, 773, 358, 3937, 916, 323, 358, 1744, 582, 1030, 825, 10435, 279, 4361, 3729, 11, 432, 572, 1246, 847, 1899, 572, 13, 1260, 2581, 1496, 6679, 311, 1191, 264, 10435, 1283, 279, 5700, 323, 566, 702, 2581, 4588, 421, 566, 1410, 633, 1550, 304, 4065, 315, 752, 3080, 3351, 892, 358, 572, 12473, 6915, 448, 714, 358, 3381, 432, 572, 16283, 429, 279, 4361, 882, 582, 18295, 700, 1573, 566, 2581, 47537, 304, 4065, 315, 752, 13, 358, 1079, 537, 2704, 421, 566, 594, 3709, 10655, 476, 566, 1101, 3171, 944, 2453, 14584, 30, 4710, 8610, 2284, 11, 438, 566, 572, 11435, 752, 700, 311, 847, 1803, 358, 4588, 1435, 421, 4297, 572, 16910, 1576, 566, 702, 1012, 15358, 2155, 13, 5301, 1172, 2033, 572, 11, 330, 9454, 11, 358, 2776, 6915, 1, 323, 1221, 432, 2684, 28759, 323, 358, 2115, 304, 847, 1803, 13, 4710, 5404, 498, 1744, 358, 1079, 1660, 4484, 88, 476, 2238, 77164, 88, 10161, 429, 3405, 30, 358, 22231, 10161, 432, 1290, 1283, 358, 4588, 432, 1576, 432, 3643, 752, 2666, 1075, 358, 614, 3347, 656, 64806, 369, 419, 5025, 13, 10696, 358, 1101, 10955, 2238, 1753, 714, 432, 702, 1012, 12182, 518, 752, 624, 13470, 26, 7687, 25, 220, 151645, 198, 151644, 77091, 198], [151644, 872, 198, 29038, 787, 4103, 952, 25, 435, 14, 85824, 198, 50328, 25, 2160, 432, 16283, 429, 419, 6519, 752, 1007, 504, 847, 41888, 5267, 2946, 25, 576, 1008, 1899, 847, 22761, 7, 17, 18, 1635, 2310, 8, 323, 7037, 7, 17, 17, 1635, 2310, 8, 1033, 7404, 323, 1340, 10457, 311, 752, 429, 1340, 4558, 3207, 944, 2400, 752, 1576, 358, 572, 2238, 2805, 320, 20, 6, 22, 34294, 20, 6, 23, 1827, 220, 2932, 374, 1172, 911, 220, 20, 6, 20, 3263, 220, 4695, 1340, 15803, 752, 264, 2696, 323, 15482, 358, 1079, 279, 1850, 3166, 311, 3512, 3537, 311, 1059, 714, 369, 1045, 2874, 11, 6832, 911, 419, 11, 2167, 6519, 752, 1007, 13, 358, 614, 2581, 5815, 2608, 448, 4113, 1008, 1091, 1246, 16217, 374, 4325, 11, 714, 13812, 1251, 21946, 2608, 448, 8170, 11, 2355, 11, 98945, 323, 1008, 6259, 429, 702, 4302, 311, 653, 448, 2608, 13, 220, 2160, 432, 3873, 315, 752, 311, 1366, 311, 1438, 705, 448, 1059, 369, 419, 25600, 1651, 1939, 40, 1414, 1181, 33390, 25600, 315, 752, 714, 358, 1079, 9016, 6519, 1007, 553, 419, 624, 13470, 26, 7687, 25, 220, 151645, 198, 151644, 77091, 198], [151644, 872, 198, 29038, 787, 4103, 952, 25, 435, 14, 85824, 198, 50328, 25, 358, 320, 17, 22, 12318, 8, 1079, 1508, 41403, 13307, 68663, 1283, 264, 342, 73930, 5729, 2820, 42930, 13, 2585, 653, 358, 10339, 847, 26775, 73118, 12, 2090, 311, 847, 25838, 320, 18, 17, 10270, 11843, 320, 55, 24410, 291, 311, 608, 81, 14, 26172, 35090, 340, 2946, 25, 7996, 2254, 11, 358, 3855, 3709, 2167, 73118, 323, 8266, 69952, 14264, 320, 16692, 17765, 68663, 911, 847, 12456, 25838, 369, 10875, 902, 2874, 11, 30199, 518, 279, 5943, 315, 264, 8896, 11, 323, 678, 315, 429, 33897, 35334, 10083, 2163, 429, 882, 11, 847, 342, 1872, 78, 3229, 752, 429, 358, 572, 4152, 311, 614, 847, 358, 4656, 6963, 13, 8670, 11, 16910, 0, 2938, 594, 3170, 358, 572, 264, 73118, 35750, 0, 425, 2584, 293, 287, 11, 143378, 29745, 11, 358, 3278, 633, 432, 12575, 323, 4297, 686, 387, 1182, 311, 4622, 382, 4498, 358, 2684, 311, 806, 5163, 369, 1128, 572, 8791, 311, 387, 264, 14021, 17635, 11, 566, 1730, 264, 3460, 3072, 304, 825, 315, 847, 65068, 550, 313, 437, 3229, 752, 429, 358, 4362, 311, 614, 264, 305, 597, 6264, 8560, 311, 633, 847, 358, 4656, 6963, 13, 1260, 13537, 279, 305, 597, 6264, 8560, 323, 279, 97641, 315, 279, 3072, 369, 279, 1790, 1899, 13, 358, 572, 81223, 323, 26115, 17176, 1717, 13, 3017, 25838, 572, 1602, 32345, 11, 19613, 705, 3309, 55935, 847, 12975, 279, 3729, 1573, 279, 10324, 11, 3697, 448, 752, 311, 279, 10668, 11, 3867, 752, 700, 369, 803, 20969, 1091, 358, 646, 1760, 773, 429, 358, 3207, 944, 614, 311, 4296, 11, 4992, 382, 2132, 594, 1012, 264, 2003, 2474, 279, 15966, 323, 11, 41662, 11, 358, 1513, 944, 2167, 2666, 894, 2686, 26115, 11, 37000, 11, 476, 68663, 13, 4695, 358, 2666, 1075, 1052, 594, 2494, 358, 2776, 2677, 2087, 311, 614, 311, 10955, 911, 13, 5542, 1128, 358, 3535, 3118, 389, 7404, 311, 1008, 3198, 11, 419, 374, 264, 8266, 429, 686, 9583, 728, 3123, 13, 1988, 1246, 653, 358, 10339, 7037, 311, 847, 8263, 323, 1077, 1435, 1414, 429, 358, 2776, 7853, 429, 358, 2776, 1660, 14264, 1290, 1431, 2041, 48127, 1075, 358, 2776, 4460, 311, 1281, 54486, 369, 1660, 264, 6587, 323, 12473, 60654, 58863, 5267, 13470, 26, 7687, 25, 220, 151645, 198, 151644, 77091, 198], [151644, 872, 198, 29038, 787, 4103, 952, 25, 435, 14, 85824, 198, 50328, 25, 54720, 342, 1945, 315, 419, 62092, 4486, 1492, 752, 320, 16, 23, 76, 8, 700, 13, 5209, 323, 9702, 498, 82630, 2946, 25, 2055, 5926, 358, 33693, 311, 279, 5492, 31733, 13759, 553, 3776, 38863, 13, 60993, 328, 7539, 13, 1988, 432, 2684, 752, 7274, 25, 330, 285, 847, 4746, 56271, 30, 3303, 358, 264, 39566, 13527, 7521, 2009, 847, 4780, 3291, 752, 358, 1079, 13221, 23115, 12562, 1694, 320, 1958, 1846, 315, 575, 879, 525, 1435, 1600, 7211, 32574, 714, 358, 614, 13919, 2581, 1012, 304, 264, 5025, 323, 614, 2677, 1012, 4238, 89, 19684, 892, 374, 6915, 13526, 633, 1483, 311, 432, 13, 1988, 358, 614, 264, 5625, 3743, 4780, 320, 9974, 279, 3550, 8, 323, 807, 265, 1817, 12456, 323, 358, 1513, 944, 1366, 311, 2666, 1075, 358, 1079, 18774, 358, 1101, 1366, 311, 387, 1968, 916, 33360, 369, 825, 13, 3555, 653, 358, 653, 25984, 1084, 374, 279, 7324, 1573, 7770, 323, 1393, 358, 1414, 432, 374, 264, 16523, 311, 1430, 311, 387, 304, 4113, 6001, 1573, 7770, 358, 2666, 1075, 1181, 825, 315, 1846, 2513, 1380, 498, 1101, 36923, 1896, 279, 31471, 13, 2980, 5489, 1492, 752, 1588, 30, 358, 1079, 14589, 369, 3259, 498, 1349, 773, 1753, 9338, 13470, 26, 7687, 25, 220, 151645, 198, 151644, 77091, 198], [151644, 872, 198, 29038, 787, 4103, 952, 25, 435, 14, 85824, 198, 50328, 25, 2157, 508, 17, 15, 12318, 60, 3432, 12264, 448, 25838, 508, 17, 18, 3183, 60, 315, 220, 17, 13, 20, 1635, 1839, 6541, 198, 2946, 25, 3017, 25838, 323, 358, 614, 458, 7897, 1839, 2272, 13, 1205, 525, 1602, 26583, 18146, 382, 15802, 11, 806, 1839, 6541, 374, 14264, 1550, 7707, 311, 10485, 13, 1084, 2167, 13798, 389, 279, 882, 315, 2254, 369, 752, 508, 71, 493, 3154, 1125, 7025, 358, 3278, 387, 1495, 311, 728, 220, 18, 3039, 264, 1899, 323, 7025, 358, 2776, 1172, 8014, 304, 3055, 264, 2003, 13, 4710, 40, 2776, 10161, 369, 9462, 1576, 358, 614, 264, 2588, 882, 40466, 1435, 2041, 1435, 8266, 17551, 553, 752, 13, 1752, 3110, 11, 1449, 882, 582, 64145, 273, 566, 374, 4558, 2677, 5001, 389, 311, 752, 13, 2411, 3729, 566, 4041, 389, 311, 752, 11, 304, 279, 6149, 315, 279, 3729, 566, 29343, 973, 1790, 311, 752, 11, 323, 16297, 311, 2525, 8630, 752, 304, 279, 6556, 13, 358, 2776, 37531, 1320, 21924, 429, 1283, 1660, 304, 264, 5025, 419, 1293, 566, 374, 2058, 1602, 6519, 389, 553, 752, 11, 714, 7025, 432, 374, 1602, 22024, 311, 2506, 705, 448, 806, 7244, 3880, 13, 4710, 40, 3003, 1012, 4460, 803, 311, 1101, 2968, 1435, 264, 472, 41, 476, 86499, 421, 566, 374, 2167, 1422, 70704, 323, 358, 2776, 537, 11, 714, 7025, 358, 2167, 1079, 1101, 37583, 60408, 504, 2272, 323, 1101, 1366, 311, 387, 64145, 832, 13, 358, 3003, 11247, 419, 311, 1435, 3807, 3039, 323, 566, 5221, 1602, 32530, 323, 16016, 323, 5302, 566, 3171, 944, 2666, 1075, 358, 1366, 311, 4486, 1435, 1934, 8206, 374, 537, 279, 1142, 518, 678, 13, 358, 646, 1490, 1246, 7025, 566, 1035, 2666, 419, 1616, 1576, 979, 566, 17064, 7025, 847, 330, 57929, 1, 1231, 2525, 3941, 56030, 476, 650, 83698, 13, 358, 1513, 944, 1366, 1435, 311, 2666, 419, 1616, 323, 432, 702, 1012, 264, 3491, 369, 264, 1393, 1431, 13, 715, 40, 1366, 311, 3960, 1246, 311, 5486, 806, 3880, 2664, 2041, 1435, 8266, 17551, 11, 323, 358, 1366, 601, 2176, 311, 387, 6247, 624, 13470, 26, 7687, 25, 220, 151645, 198, 151644, 77091, 198]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "batch_inputs = dataset[\"train\"][0:10]\n",
    "print(f\"batch_inputs: {batch_inputs}\")\n",
    "input_ids = batch_inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ba4115db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input_ids shape: torch.Size([4, 436])\n",
      "Batch input_ids: tensor([[151644,    872,    198,  ..., 151644,  77091,    198],\n",
      "        [151643, 151643, 151643,  ..., 151644,  77091,    198],\n",
      "        [151643, 151643, 151643,  ..., 151644,  77091,    198],\n",
      "        [151643, 151643, 151643,  ..., 151644,  77091,    198]])\n",
      "Batch attention_mask shape: torch.Size([4, 436])\n",
      "Batch attention_mask: tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "prompts_dataloader = DataLoader(\n",
    "    dataset[\"train\"],\n",
    "    batch_size=4,\n",
    "    collate_fn=DataCollatorWithPadding(tokenizer=tokenizer, padding='longest'),\n",
    ")\n",
    "\n",
    "batch = next(iter(prompts_dataloader))\n",
    "print(f\"Batch input_ids shape: {batch['input_ids'].shape}\")\n",
    "print(f\"Batch input_ids: {batch['input_ids']}\")\n",
    "print(f\"Batch attention_mask shape: {batch['attention_mask'].shape}\")\n",
    "print(f\"Batch attention_mask: {batch['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2ca2c25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen3-1.7B\",\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0b462e45",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/align_trl/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/align_trl/lib/python3.12/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/align_trl/lib/python3.12/site-packages/transformers/generation/utils.py:2829\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2827\u001b[39m     probs = nn.functional.softmax(next_token_scores, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m   2828\u001b[39m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2829\u001b[39m     next_tokens = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m.squeeze(\u001b[32m1\u001b[39m)\n\u001b[32m   2830\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2831\u001b[39m     next_tokens = torch.argmax(next_token_scores, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=batch['input_ids'].to(model.device),\n",
    "        attention_mask=batch['attention_mask'].to(model.device),\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1be49bf",
   "metadata": {},
   "source": [
    "## 方法2: 使用 vLLM (最快，适合大规模生成)\n",
    "\n",
    "vLLM 提供了更高的吞吐量，特别适合大规模生成任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be9b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 vLLM (需要先安装: pip install vllm)\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# 初始化 vLLM 模型，自动使用所有可用GPU\n",
    "llm = LLM(\n",
    "    model=\"Qwen/Qwen3-1.7B\",\n",
    "    trust_remote_code=True,\n",
    "    tensor_parallel_size=torch.cuda.device_count(),  # 使用所有GPU\n",
    "    dtype=\"bfloat16\",\n",
    ")\n",
    "\n",
    "# 设置生成参数\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "print(f\"Using {torch.cuda.device_count()} GPUs for vLLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4f9970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 vLLM 批量生成\n",
    "def generate_with_vllm(examples):\n",
    "    \"\"\"使用 vLLM 批量生成\"\"\"\n",
    "    # 格式化提示\n",
    "    formatted_prompts = []\n",
    "    for prompt_messages in examples[\"prompt\"]:\n",
    "        formatted = tokenizer.apply_chat_template(\n",
    "            prompt_messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "        formatted_prompts.append(formatted)\n",
    "    \n",
    "    # 批量生成（vLLM 会自动处理批次）\n",
    "    outputs = llm.generate(formatted_prompts, sampling_params)\n",
    "    \n",
    "    # 提取生成的文本\n",
    "    completions = [output.outputs[0].text for output in outputs]\n",
    "    \n",
    "    return {\"completion\": completions}\n",
    "\n",
    "# 对数据集进行生成\n",
    "vllm_results = dataset.map(\n",
    "    generate_with_vllm,\n",
    "    batched=True,\n",
    "    batch_size=64,  # vLLM 可以处理更大的批次\n",
    ")\n",
    "\n",
    "print(f\"vLLM generation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "align_trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
